{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data Warehouse\n",
    "### Data Engineering Project\n",
    "\n",
    "#### Project Summary\n",
    "This project aims to be able to answers questions around US immigration such as what are the most popular cities for immigration,what is count of immigrants by visa type and city in descending order of count,what is the average temperature of the city where immigrant count is low or high on a perticulat year, What is the percentage of foreign born in a city where the immigrant is moving to.. We extract data from three different data sources, the I94 immigration dataset of 2016, city temperature data from Kaggle and US city demographic data from OpenSoft. We have designed 3 dimension tables: dimTemperature, dimImmigration , dimDemographics and one fact table: factImmigration. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col, udf, year, month, avg,monotonically_increasing_id\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The goal of this project is ingest data from three different data sources and create fact and dimension table to be able to do analysis on US immigration using factors of city, average temperature, city demographics and seasonality.We use Spark for ETL jobs and store the results in parquet for downstream analysis by a Data Analyst or Data Scientist.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "**I94 Immigration Data**: comes from the U.S. National Tourism and Trade Office and contains various statistics on international visitor arrival in USA and comes from the US National Tourism and Trade Office. The dataset contains data from 2016.<br>\n",
    "**World Temperature Data**: comes from Kaggle and contains average weather temperatures by city. <br>\n",
    "**U.S.City Demographic Data**: comes from OpenSoft and contains information about the demographics of all US cities such as average age, male and female population. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readDataFromSource():\n",
    "    \"\"\" This function reads data from Source and returns it as a dataframe \"\"\"\n",
    "    # Read temperature data\n",
    "    temparatureData = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "    # Read immigeration data\n",
    "    immigrationData = spark.read.load('./sas_data')\n",
    "    # Read demographics data\n",
    "    demographyData = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"true\").load( \"us-cities-demographics.csv\")\n",
    "    return temparatureData,immigrationData,demographyData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def convert_datetime(x):\n",
    "    \"\"\" This user-defined function converts udf to convert SAS date to PySpark date \"\"\"\n",
    "    if x:\n",
    "        return (datetime(1960, 1, 1).date() + timedelta(x)).isoformat()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def immigrationStaging(immigrationData):\n",
    "    \"\"\" This function removes null from essential columns and creates columns with alias name for immigration Data . The output is a Staging Layer for immigrationData\"\"\"\n",
    "    immigrationDatadf = immigrationData.filter(immigrationData.i94addr.isNotNull()).filter(immigrationData.cicid.isNotNull()).withColumn(\"Year\",col(\"i94yr\").cast(\"integer\")).withColumn(\"Month\",col(\"i94mon\").cast(\"integer\"))\n",
    "    cleaned_immigrationData = immigrationDatadf.withColumn(\"arrdate\", convert_datetime(immigrationDatadf.arrdate))\n",
    "    staging_immigrationData = cleaned_immigrationData.select(col(\"cicid\").alias(\"id\"), \n",
    "                                           col(\"arrdate\").alias(\"date\"),\n",
    "                                           col(\"i94port\").alias(\"city_code\"),\n",
    "                                           col(\"i94addr\").alias(\"state_code\"),\n",
    "                                           col(\"i94bir\").alias(\"age\"),\n",
    "                                           col(\"Year\").alias(\"year_of_arrival\"),\n",
    "                                           col(\"Month\").alias(\"month_of_arrival\"),\n",
    "                                           col(\"gender\").alias(\"gender\"),\n",
    "                                           col(\"visatype\").alias(\"visa_type\"),\n",
    "                                           \"count\").drop_duplicates()\n",
    "\n",
    "    \n",
    "    print(\"immigrationStaging successful\")\n",
    "    return staging_immigrationData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def temperatureStaging(temparatureData): \n",
    "    \"\"\" This function removes null from essential columns and casts datatype for aggregation. The output is a cleansed form of temparatureData\"\"\"\n",
    "    temparatureData_Cleansed = temparatureData.filter(temparatureData.Country=='United States').filter(temparatureData.AverageTemperatureUncertainty.isNotNull()).filter(temparatureData.AverageTemperature.isNotNull())\n",
    "    temparatureData_Aggrigated=temparatureData_Cleansed.withColumn(\"AvgTemp\",col(\"AverageTemperature\").cast(\"float\")).withColumn(\"AvgDifferenceinTemp\",col(\"AverageTemperatureUncertainty\").cast(\"float\"))\n",
    "    stageTemperature=temparatureData_Aggrigated.groupBy(\"Country\",\"City\").agg({'AvgTemp':'avg', 'AvgDifferenceinTemp':'avg'})\n",
    "    staging_TemperatureData=stageTemperature.select(\"Country\",\"City\",col(\"avg(AvgTemp)\").alias(\"AvgTemp\"),col(\"avg(AvgDifferenceinTemp)\").alias(\"AvgDifferenceinTemp\"))\n",
    "    staging_TemperatureData.createOrReplaceTempView(\"dimTemperature\")\n",
    "    print(\"temperatureStaging successful\")\n",
    "    \n",
    "    return staging_TemperatureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def demographyStaging(demographyData):\n",
    "    \"\"\" This function aggrigates columns in terms of percentage and stores them in a new column. The output is a Staging Layer for demographyData\"\"\"\n",
    "    stage_demographyData = demographyData.withColumn(\"median_age\", demographyData['Median Age']) \\\n",
    "        .withColumn(\"pcnt_male_pop\", (demographyData['Male Population'] / demographyData['Total Population']) * 100) \\\n",
    "        .withColumn(\"pcnt_female_pop\", (demographyData['Female Population'] / demographyData['Total Population']) * 100) \\\n",
    "        .withColumn(\"pcnt_foreign_born\", (demographyData['Foreign-born'] / demographyData['Total Population']) * 100).withColumn(\"state_code\", (demographyData['State Code'])).withColumn(\"total_pop\", (demographyData['Total Population'])) \n",
    "\n",
    "    print(\"demographyStaging successful\")\n",
    "    return stage_demographyData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def immigrationDimension(staging_Temperature):\n",
    "    \"\"\" This function loads staging_immigrationData into dimImmigration dimension table\"\"\"\n",
    "    staging_immigrationData.createOrReplaceTempView(\"dimImmigration\")\n",
    "    dimImmigration = spark.sql('''SELECT id,state_code,city_code,visa_type,year_of_arrival,month_of_arrival FROM dimImmigration''')\n",
    "    print(\"immigrationDimension successful\")\n",
    "    return dimImmigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def temperatureDimension(staging_TemperatureData):\n",
    "    \"\"\" This function loads temparatureData_Cleansed , Performs Aggrigation and loads it into dimTemperature dimension table\"\"\"\n",
    "    staging_TemperatureData.createOrReplaceTempView(\"dimTemperature\")\n",
    "    dimTemperature = spark.sql('''SELECT * FROM dimTemperature''')\n",
    "    print(\"temperatureDimension successful\")\n",
    "    return dimTemperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def demographyDimension(stage_demographyData):\n",
    "    \"\"\" This function loads stage_demographyData into dimDemography dimension table\"\"\"\n",
    "    stage_demographyData.createOrReplaceTempView(\"dimDemography\")\n",
    "    dimDemography = spark.sql('''SELECT state_code,state,city,median_age,pcnt_male_pop,pcnt_female_pop,pcnt_foreign_born,total_pop FROM dimDemography''')\n",
    "    print(\"demographyDimension successful\")\n",
    "    return dimDemography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "**DIMENSION TABLES**\n",
    "\n",
    "1.**dimImmigration** : It contains immigration events\n",
    "\n",
    "* id               - Unique Identifier\n",
    "* state_code       - State Code\n",
    "* city_code        - City Code \n",
    "* visa_type        - Type of visa issues\n",
    "* year_of_arrival  - Year of Arrival to US\n",
    "* month_of_arrival - Month of Arrival to US\n",
    "\n",
    "2.**dimTemperature** :  It contains average temparature city in US\n",
    "\n",
    "* Country             - Country Name\n",
    "* City                - City Name\n",
    "* AvgTemp             - Average Temperature in City\n",
    "* AvgDifferenceinTemp - Average Variation In Temperature\n",
    "\n",
    "3.**dimDemography** : It has information on Demographic Statistics\n",
    "\n",
    "* state_code         - State Code\n",
    "* city               - City Name\n",
    "* median_age         - Median age of people in city\n",
    "* pcnt_male_pop      - Percentage of Male Population\n",
    "* pcnt_female_pop    - Percentage of Female Population\n",
    "* pcnt_foreign_born  - Percentage of People who are born outside US \n",
    "* total_pop          - Total Polulation\n",
    "\n",
    "4.**factImmigration** : The Fact table gives the count of entry into US soil.\n",
    "\n",
    "* factImmigration_pk  - Unique Identifier\n",
    "* id                  - Unique Identifier of dimImmigration\n",
    "* city                - City Name\n",
    "* state_code          - State Code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Three Staging tables are created from raw data viz staging_immigrationData,staging_TemperatureData and stage_demographyData\n",
    "\n",
    "2. dimTemperature is created by reading data from csv file and aggregated data after cleaning from staging_TemperatureData \n",
    "\n",
    "3. dimImmigration is created from list of sas files and cleaned from staging_immigrationData\n",
    "\n",
    "4. dimDemography is created by reading data from csv file after cleaning tables from stage_demographyData\n",
    "\n",
    "5. factImmigration is created by joining staging_immigrationData,dimDemography and dimTemperature tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# factImmigration is created by joining staging_immigrationData,dimDemography and dimTemperature tables\n",
    "\n",
    "def immigrationFact(staging_immigrationData,stage_demographyData,dimTemperature):\n",
    "    \"\"\"In This function factImmigration is created by joining staging_immigrationData,dimDemography and dimTemperature tables\"\"\"\n",
    "    staging_immigrationData.createOrReplaceTempView(\"stgImmigration\")\n",
    "    stage_demographyData.createOrReplaceTempView(\"dimDemography\")\n",
    "    dimTemperature.createOrReplaceTempView(\"dimTemperature\")\n",
    "    factImmigration = spark.sql('''\n",
    "    SELECT stgImmigration.id,\n",
    "           dimTemperature.City,\n",
    "           dimDemography.state_code,\n",
    "           stgImmigration.count\n",
    "    FROM dimTemperature  \n",
    "    JOIN dimDemography  ON (dimTemperature.city = dimDemography.city)\n",
    "    JOIN stgImmigration ON ( dimDemography.state_code=stgImmigration.state_code)\n",
    "\n",
    "    ''')\n",
    "    # Adding Sequencial Primary Key to Fact Table\n",
    "    factImmigration=factImmigration.withColumn(\"factImmigration_pk\", monotonically_increasing_id()) \n",
    "    print(\"factImmigration successful\")\n",
    "    return factImmigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def writeTableAsParquet(dimTemperature,dimDemography,dimImmigration,factImmigration):\n",
    "    \"\"\" This function is used to write all the fact and dimension tables into Parquet file\"\"\"\n",
    "    spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "    # Write Dimention Tables as Parquet File\n",
    "    dimTemperature.write.mode(\"append\").partitionBy(\"City\").parquet(\"/target/dimTemperature.parquet\")\n",
    "    # Write Dimention Tables as Parquet File\n",
    "    dimDemography.write.mode(\"append\").partitionBy(\"city\").parquet(\"/target/dimDemography.parquet\")\n",
    "    # Write Dimention Tables as Parquet File\n",
    "    dimImmigration.write.mode(\"append\").partitionBy(\"city_code\").parquet(\"/target/dimImmigration.parquet\")\n",
    "    # Write Dimention Tables as Parquet File\n",
    "    factImmigration.write.mode(\"append\").partitionBy(\"City\").parquet(\"/target/factImmigration.parquet\")\n",
    "    print(\"writeTableAsParquet successful\")\n",
    "    print(\"Data Written Successfully\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## PIPELINE \n",
    "#### P.1 Data Reading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immigrationStaging successful\n",
      "temperatureStaging successful\n",
      "demographyStaging successful\n"
     ]
    }
   ],
   "source": [
    "temparatureData,immigrationData,demographyData = readDataFromSource()\n",
    "staging_immigrationData = immigrationStaging(immigrationData)\n",
    "staging_TemperatureData = temperatureStaging(temparatureData)\n",
    "stage_demographyData=demographyStaging(demographyData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### P.2 Fact and Dimension Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immigrationDimension successful\n",
      "temperatureDimension successful\n",
      "demographyDimension successful\n",
      "factImmigration successful\n",
      "writeTableAsParquet successful\n",
      "Data Written Successfully\n"
     ]
    }
   ],
   "source": [
    "dimImmigration = immigrationDimension(staging_immigrationData)\n",
    "dimTemperature = temperatureDimension(staging_TemperatureData)\n",
    "dimDemography = demographyDimension(stage_demographyData)\n",
    "factImmigration = immigrationFact(staging_immigrationData,stage_demographyData,dimTemperature)\n",
    "writeTableAsParquet(dimTemperature,dimDemography,dimImmigration,factImmigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dataQualityCountCheck():\n",
    "    \"\"\" This function is used to validate the count of source and target columns\"\"\"\n",
    "    count=0\n",
    "    if staging_immigrationData.count() == dimImmigration.count():\n",
    "        count +=1\n",
    "    if stage_demographyData.count() == dimDemography.count():\n",
    "        count +=1\n",
    "    if staging_TemperatureData.count() == dimTemperature.count():\n",
    "        count +=1\n",
    "    if count ==3:\n",
    "        print(\"Data Quality Check Successful ! Data at Source and Target Equal\")\n",
    "    else:\n",
    "        print(\"Data Quality Check Failed ! Data at Source and Target Are Not Equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def DataQualityNullCheck():\n",
    "    \"\"\" This function is used to validate if null is present in primary key\"\"\"\n",
    "    count=0\n",
    "    if dimImmigration.where(col(\"id\").isNull()).count() == 0:\n",
    "        count +=1\n",
    "    if dimTemperature.where(col(\"City\").isNull()).count() == 0:\n",
    "        count +=1\n",
    "    if dimDemography.where(col(\"state_code\").isNull()).count() == 0:\n",
    "        count +=1\n",
    "    if count == 3:\n",
    "        print(\"Data Quality Check Successful ! No Null present in Primary Key Columns\")\n",
    "    else:\n",
    "        print(\"Data Quality Check Failed ! Null present in Primary Key Columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Check Successful ! Data at Source and Target Equal\n"
     ]
    }
   ],
   "source": [
    "dataQualityCountCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Check Successful ! No Null present in Primary Key Columns\n"
     ]
    }
   ],
   "source": [
    "DataQualityNullCheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. \n",
    "\n",
    "**DIMENSION TABLES**\n",
    "\n",
    "1.**dimImmigration** : It contains immigration events\n",
    "\n",
    "* id               - Unique Identifier\n",
    "* state_code       - State Code\n",
    "* city_code        - City Code \n",
    "* visa_type        - Type of visa issues\n",
    "* year_of_arrival  - Year of Arrival to US\n",
    "* month_of_arrival - Month of Arrival to US\n",
    "\n",
    "2.**dimTemperature** :  It contains average temparature city in US\n",
    "\n",
    "* Country             - Country Name\n",
    "* City                - City Name\n",
    "* AvgTemp             - Average Temperature in City\n",
    "* AvgDifferenceinTemp - Average Variation In Temperature\n",
    "\n",
    "3.**dimDemography** : It has information on Demographic Statistics\n",
    "\n",
    "* state_code         - State Code\n",
    "* city               - City Name\n",
    "* median_age         - Median age of people in city\n",
    "* pcnt_male_pop      - Percentage of Male Population\n",
    "* pcnt_female_pop    - Percentage of Female Population\n",
    "* pcnt_foreign_born  - Percentage of People who are born outside US \n",
    "* total_pop          - Total Polulation\n",
    "\n",
    "4.**factImmigration** : The Fact table gives the count of entry into US soil.\n",
    "\n",
    "* factImmigration_pk  - Unique Identifier\n",
    "* id                  - Unique Identifier of dimImmigration\n",
    "* city                - City Name\n",
    "* state_code          - State Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* **Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "PySpark is chosen for this project as it is known for processing large amount of data fast (with in-memory compute), scale easily with additional worker nodes, with ability to digest different data formats (e.g. SAS, Parquet, CSV), and integrate nicely with cloud storage like S3 and warehouse like Redshift.Python provides an additional edge to spark native scala with its enormous library size.The Fact and Dimention tables are created with star schema .The star schema’s goal is to speed up read queries and analysis for massive amounts of data contained in diverse databases with different source schemas. The star schema achieves this goal through the “denormalization” of the data within the network of dimension tables.\n",
    "* **Propose how often the data should be updated and why.**\n",
    "It Depends on the Data Availability at source and Reporting Cycle. If the data at source is available bi-monthly we can perform a complete refresh once a fortnight.\n",
    "* **Write a description of how you would approach the problem differently under the following scenarios:**\n",
    " * **The data was increased by 100x.**\n",
    "We can consider spinning up larger instances of EC2s hosting Spark and/or additional Spark work nodes. With added capacity arising from either vertical scaling or horizontal scaling, we should be able to accelerate processing time.\n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "We can consider using Airflow/Oozie to schedule and automate the data pipeline jobs.\n",
    " * **The database needed to be accessed by 100+ people.**\n",
    "We can use AWS Redshift With Concurrency Scaling feature, which can support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance. When concurrency scaling is enabled, Amazon Redshift automatically adds additional cluster capacity when you need it to process an increase in concurrent read queries. Write operations continue as normal on your main cluster. Users always see the most current data, whether the queries run on the main cluster or on a concurrency scaling cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Sample Query - what is count of immigrants by visa type and city in descending order of count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+\n",
      "|            city|visa_type|count(id)|\n",
      "+----------------+---------+---------+\n",
      "|    Jacksonville|       B2|  1633525|\n",
      "|       Rochester|       WT|  1597890|\n",
      "|Port Saint Lucie|       B2|  1595460|\n",
      "|      Clearwater|       B2|  1595460|\n",
      "|         Orlando|       B2|  1595460|\n",
      "|      Cape Coral|       B2|  1595460|\n",
      "|     Tallahassee|       B2|  1595460|\n",
      "|   Coral Springs|       B2|  1595460|\n",
      "| Fort Lauderdale|       B2|  1595460|\n",
      "|     Gainesville|       B2|  1595460|\n",
      "|  Pembroke Pines|       B2|  1595460|\n",
      "|           Miami|       B2|  1595460|\n",
      "|           Tampa|       B2|  1595460|\n",
      "|Saint Petersburg|       B2|  1595460|\n",
      "|       Hollywood|       B2|  1595460|\n",
      "|         Yonkers|       WT|  1585910|\n",
      "|        Syracuse|       WT|  1585910|\n",
      "|        New York|       WT|  1585910|\n",
      "|         Buffalo|       WT|  1585910|\n",
      "|    Jacksonville|       WT|  1278305|\n",
      "+----------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "factImmigration.createOrReplaceTempView(\"factImmigration\")\n",
    "dimImmigration.createOrReplaceTempView(\"dimImmigration\")\n",
    "\n",
    "Query = spark.sql('''\n",
    "    SELECT factImmigration.city,dimImmigration.visa_type,count(factImmigration.id) \n",
    "    FROM dimImmigration JOIN  factImmigration ON (dimImmigration.id == factImmigration.id) GROUP BY dimImmigration.visa_type,factImmigration.city ORDER BY count(factImmigration.id) DESC ''')\n",
    "\n",
    "Query.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
