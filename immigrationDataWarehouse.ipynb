{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data Warehouse\n",
    "### Data Engineering Project\n",
    "\n",
    "#### Project Summary\n",
    "This project aims to be able to answers questions around US immigration. We extract data from three different data sources, the I94 immigration dataset of 2016, city temperature data from Kaggle and US city demographic data from OpenSoft. We have designed 3 dimension tables: dimTemperature, dimImmigration , dimDemographics and one fact table: factImmigration. We use Spark for ETL jobs and store the results in parquet for downstream analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, year, month, col, isnull,avg,monotonically_increasing_id, isnan, when, count\n",
    "import numpy as np\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The goal of this project is ingest data from three different data sources and create fact and dimension table to be able to do analysis on US immigration using factors of city, average temperature, city demographics and seasonality.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "**I94 Immigration Data**: comes from the U.S. National Tourism and Trade Office and contains various statistics on international visitor arrival in USA and comes from the US National Tourism and Trade Office. The dataset contains data from 2016.<br>\n",
    "**World Temperature Data**: comes from Kaggle and contains average weather temperatures by city. <br>\n",
    "**U.S.City Demographic Data**: comes from OpenSoft and contains information about the demographics of all US cities such as average age, male and female population. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "# Read temperature data\n",
    "temparatureData = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "# Read immigeration data\n",
    "immigrationData = spark.read.load('./sas_data')\n",
    "# Read demographics data\n",
    "demographyData = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"true\").load( \"us-cities-demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigrationData.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temparatureData.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographyData.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create udf to convert SAS date to PySpark date \n",
    "@udf(StringType())\n",
    "def convert_datetime(x):\n",
    "    if x:\n",
    "        return (datetime(1960, 1, 1).date() + timedelta(x)).isoformat()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Immigration Data Cleaning and Staging\n",
    "\n",
    "#creating alias name \n",
    "immigrationDatadf = immigrationData.filter(immigrationData.i94addr.isNotNull()).filter(immigrationData.cicid.isNotNull()).withColumn(\"Year\",col(\"i94yr\").cast(\"integer\")).withColumn(\"Month\",col(\"i94mon\").cast(\"integer\"))\n",
    "cleaned_immigrationData = immigrationDatadf.withColumn(\"arrdate\", convert_datetime(immigrationDatadf.arrdate))\n",
    "staging_immigrationData = cleaned_immigrationData.select(col(\"cicid\").alias(\"id\"), \n",
    "                                       col(\"arrdate\").alias(\"date\"),\n",
    "                                       col(\"i94port\").alias(\"city_code\"),\n",
    "                                       col(\"i94addr\").alias(\"state_code\"),\n",
    "                                       col(\"i94bir\").alias(\"age\"),\n",
    "                                       col(\"Year\").alias(\"year_of_arrival\"),\n",
    "                                       col(\"Month\").alias(\"month_of_arrival\"),\n",
    "                                       col(\"gender\").alias(\"gender\"),\n",
    "                                       col(\"visatype\").alias(\"visa_type\"),\n",
    "                                       \"count\").drop_duplicates()\n",
    "\n",
    "staging_immigrationData.show()\n",
    "staging_immigrationData.createOrReplaceTempView(\"stgImmigration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Immigration Data Cleaning\n",
    "temparatureData_Cleansed = temparatureData.filter(temparatureData.Country=='United States').filter(temparatureData.AverageTemperatureUncertainty.isNotNull()).filter(temparatureData.AverageTemperature.isNotNull())\n",
    "temparatureData_Cleansed=temparatureData_Cleansed.withColumn(\"AvgTemp\",col(\"AverageTemperature\").cast(\"float\")).withColumn(\"AvgDifferenceinTemp\",col(\"AverageTemperatureUncertainty\").cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Demography data Staging\n",
    "stage_demographyData = demographyData.withColumn(\"median_age\", demographyData['Median Age']) \\\n",
    "    .withColumn(\"pcnt_male_pop\", (demographyData['Male Population'] / demographyData['Total Population']) * 100) \\\n",
    "    .withColumn(\"pcnt_female_pop\", (demographyData['Female Population'] / demographyData['Total Population']) * 100) \\\n",
    "    .withColumn(\"pcnt_foreign_born\", (demographyData['Foreign-born'] / demographyData['Total Population']) * 100).withColumn(\"state_code\", (demographyData['State Code'])).withColumn(\"total_pop\", (demographyData['Total Population'])) \n",
    "\n",
    "\n",
    "stage_demographyData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating Dimension Table\n",
    "staging_immigrationData.createOrReplaceTempView(\"dimImmigration\")\n",
    "dimImmigration = spark.sql('''SELECT id,state_code,city_code,visa_type,year_of_arrival,month_of_arrival FROM dimImmigration''')\n",
    "dimImmigration.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating Dimension Table\n",
    "dimTemperature=temparatureData_Cleansed.groupBy(\"Country\",\"City\").agg({'AvgTemp':'avg', 'AvgDifferenceinTemp':'avg'})\n",
    "dimTemperature=dimTemperature.select(\"Country\",\"City\",col(\"avg(AvgTemp)\").alias(\"AvgTemp\"),col(\"avg(AvgDifferenceinTemp)\").alias(\"AvgDifferenceinTemp\"))\n",
    "dimTemperature.createOrReplaceTempView(\"dimTemperature\")\n",
    "dimTemperature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating Dimension Table\n",
    "stage_demographyData.createOrReplaceTempView(\"dimDemography\")\n",
    "dimDemography = spark.sql('''SELECT state_code,state,city,median_age,pcnt_male_pop,pcnt_female_pop,pcnt_foreign_born,total_pop FROM dimDemography''')\n",
    "dimDemography.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "**DIMENSION TABLES**\n",
    "\n",
    "1.**dimImmigration** : It contains immigration events\n",
    "\n",
    "* id               - Unique Identifier\n",
    "* state_code       - State Code\n",
    "* city_code        - City Code \n",
    "* visa_type        - Type of visa issues\n",
    "* year_of_arrival  - Year of Arrival to US\n",
    "* month_of_arrival - Month of Arrival to US\n",
    "\n",
    "2.**dimTemperature** :  It contains average temparature city in US\n",
    "\n",
    "* Country             - Country Name\n",
    "* City                - City Name\n",
    "* AvgTemp             - Average Temperature in City\n",
    "* AvgDifferenceinTemp - Average Variation In Temperature\n",
    "\n",
    "3.**dimDemography** : It has information on Demographic Statistics\n",
    "\n",
    "* state_code         - State Code\n",
    "* city               - City Name\n",
    "* median_age         - Median age of people in city\n",
    "* pcnt_male_pop      - Percentage of Male Population\n",
    "* pcnt_female_pop    - Percentage of Female Population\n",
    "* pcnt_foreign_born  - Percentage of People who are born outside US \n",
    "* total_pop          - Total Polulation\n",
    "\n",
    "4.**factImmigration** : The Fact table gives the count of entry into US soil.\n",
    "\n",
    "* factImmigration_pk  - Unique Identifier\n",
    "* id                  - Unique Identifier of dimImmigration\n",
    "* city                - City Name\n",
    "* state_code          - State Code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1.dimTemperature is created by reading data from csv file and aggregated data after cleaning (Refer Step 2)\n",
    "\n",
    "2.dimImmigration is created from list of sas files and cleaned (Refer Step 2)\n",
    "\n",
    "3.dimDemography is created by reading data from csv file after cleaning tables (Refer Step 2)\n",
    "\n",
    "4.factImmigration is created by joining staging_immigrationData,dimDemography and dimTemperature tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# factImmigration is created by joining staging_immigrationData,dimDemography and dimTemperature tables\n",
    "factImmigration = spark.sql('''\n",
    "SELECT stgImmigration.id,\n",
    "       dimTemperature.City,\n",
    "       dimDemography.state_code,\n",
    "       stgImmigration.count\n",
    "FROM dimTemperature  \n",
    "JOIN dimDemography  ON (dimTemperature.city = dimDemography.city)\n",
    "JOIN stgImmigration ON ( dimDemography.state_code=stgImmigration.state_code)\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Adding Sequencial Primary Key to Fact Table\n",
    "factImmigration=factImmigration.withColumn(\"factImmigration_pk\", monotonically_increasing_id()) \n",
    "factImmigration.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write Dimention Tables as Parquet File\n",
    "dimTemperature.write.mode(\"append\").partitionBy(\"City\").parquet(\"/target/dimTemperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write Dimention Tables as Parquet File\n",
    "dimDemography.write.mode(\"append\").partitionBy(\"city\").parquet(\"/target/dimDemography.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write Dimention Tables as Parquet File\n",
    "dimImmigration.write.mode(\"append\").partitionBy(\"city_code\").parquet(\"/target/dimImmigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write Dimention Tables as Parquet File\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "factImmigration.write.mode(\"append\").partitionBy(\"City\").parquet(\"/target/factImmigration.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check dimTemperature counts\n",
    "dimTemperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check dimImmigration counts\n",
    "dimImmigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check dimDemography counts\n",
    "dimDemography.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check factImmigration counts\n",
    "factImmigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check dimTemperature primary key has null\n",
    "dimTemperature.select([count(when(isnan('City'),True))]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check dimImmigration primary key has null\n",
    "dimImmigration.select([count(when(isnan('id'),True))]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check dimDemography primary key has null\n",
    "dimDemography.select([count(when(isnan('state_code'),True))]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. \n",
    "\n",
    "**DIMENSION TABLES**\n",
    "\n",
    "1.**dimImmigration** : It contains immigration events\n",
    "\n",
    "* id               - Unique Identifier\n",
    "* state_code       - State Code\n",
    "* city_code        - City Code \n",
    "* visa_type        - Type of visa issues\n",
    "* year_of_arrival  - Year of Arrival to US\n",
    "* month_of_arrival - Month of Arrival to US\n",
    "\n",
    "2.**dimTemperature** :  It contains average temparature city in US\n",
    "\n",
    "* Country             - Country Name\n",
    "* City                - City Name\n",
    "* AvgTemp             - Average Temperature in City\n",
    "* AvgDifferenceinTemp - Average Variation In Temperature\n",
    "\n",
    "3.**dimDemography** : It has information on Demographic Statistics\n",
    "\n",
    "* state_code         - State Code\n",
    "* city               - City Name\n",
    "* median_age         - Median age of people in city\n",
    "* pcnt_male_pop      - Percentage of Male Population\n",
    "* pcnt_female_pop    - Percentage of Female Population\n",
    "* pcnt_foreign_born  - Percentage of People who are born outside US \n",
    "* total_pop          - Total Polulation\n",
    "\n",
    "4.**factImmigration** : The Fact table gives the count of entry into US soil.\n",
    "\n",
    "* factImmigration_pk  - Unique Identifier\n",
    "* id                  - Unique Identifier of dimImmigration\n",
    "* city                - City Name\n",
    "* state_code          - State Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* **Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "PySpark is chosen for this project as it is known for processing large amount of data fast (with in-memory compute), scale easily with additional worker nodes, with ability to digest different data formats (e.g. SAS, Parquet, CSV), and integrate nicely with cloud storage like S3 and warehouse like Redshift.Python provides an additional edge to spark native scala with its enormous library size.\n",
    "* **Propose how often the data should be updated and why.**\n",
    "It Depends on the Data Availability at source and Reporting Cycle. If the data at source is available bi-monthly we can perform a complete refresh once a fortnight.\n",
    "* **Write a description of how you would approach the problem differently under the following scenarios:**\n",
    " * **The data was increased by 100x.**\n",
    "We can consider spinning up larger instances of EC2s hosting Spark and/or additional Spark work nodes. With added capacity arising from either vertical scaling or horizontal scaling, we should be able to accelerate processing time.\n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "We can consider using Airflow/Oozie to schedule and automate the data pipeline jobs.\n",
    " * **The database needed to be accessed by 100+ people.**\n",
    "We can use AWS Redshift With Concurrency Scaling feature, which can support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance. When concurrency scaling is enabled, Amazon Redshift automatically adds additional cluster capacity when you need it to process an increase in concurrent read queries. Write operations continue as normal on your main cluster. Users always see the most current data, whether the queries run on the main cluster or on a concurrency scaling cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
